{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "X3XY50Jxb51b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-$(lsb_release -c -s) main\" \\\n",
        "  | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!sudo apt-get -q update\n",
        "!sudo apt-get -q install gcsfuse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNRt_iGBcH3_",
        "outputId": "874ce510-b1e7-461e-a978-b467cb477711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb http://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "100  1022  100  1022    0     0  14502      0 --:--:-- --:--:-- --:--:-- 14600\n",
            "OK\n",
            "Hit:1 https://cli.github.com/packages stable InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://packages.cloud.google.com/apt gcsfuse-jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists...\n",
            "W: http://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "gcsfuse is already the newest version (3.4.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/gcs_mount\n",
        "!gcsfuse --implicit-dirs data_storage_bucket1 /content/gcs_mount\n",
        "%cd gcs_mount\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ePozrIpcrBR",
        "outputId": "1242bd6d-6329-409b-f199-d11b024d84f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/gcs_mount’: File exists\n",
            "{\"timestamp\":{\"seconds\":1760572745,\"nanos\":726090166},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/3.4.0 (Go version go1.24.5) for app \\\"\\\" using mount point: /content/gcs_mount\\n\"}\n",
            "{\"timestamp\":{\"seconds\":1760572745,\"nanos\":726127279},\"severity\":\"INFO\",\"message\":\"GCSFuse config\",\"config\":{\"AppName\":\"\",\"CacheDir\":\"\",\"CloudProfiler\":{\"AllocatedHeap\":true,\"Cpu\":true,\"Enabled\":false,\"Goroutines\":false,\"Heap\":true,\"Label\":\"gcsfuse-0.0.0\",\"Mutex\":false},\"Debug\":{\"ExitOnInvariantViolation\":false,\"Fuse\":false,\"Gcs\":false,\"LogMutex\":false},\"DisableAutoconfig\":false,\"EnableAtomicRenameObject\":true,\"EnableGoogleLibAuth\":false,\"EnableHns\":true,\"EnableNewReader\":true,\"FileCache\":{\"CacheFileForRangeRead\":false,\"DownloadChunkSizeMb\":200,\"EnableCrc\":false,\"EnableODirect\":false,\"EnableParallelDownloads\":false,\"ExperimentalExcludeRegex\":\"\",\"ExperimentalParallelDownloadsDefaultOn\":true,\"MaxParallelDownloads\":24,\"MaxSizeMb\":-1,\"ParallelDownloadsPerFile\":16,\"WriteBufferSize\":4194304},\"FileSystem\":{\"DirMode\":\"755\",\"DisableParallelDirops\":false,\"ExperimentalEnableDentryCache\":false,\"ExperimentalEnableReaddirplus\":false,\"FileMode\":\"644\",\"FuseOptions\":[],\"Gid\":-1,\"IgnoreInterrupts\":true,\"KernelListCacheTtlSecs\":0,\"PreconditionErrors\":true,\"RenameDirLimit\":0,\"TempDir\":\"\",\"Uid\":-1},\"Foreground\":false,\"GcsAuth\":{\"AnonymousAccess\":false,\"KeyFile\":\"\",\"ReuseTokenFromUrl\":true,\"TokenUrl\":\"\"},\"GcsConnection\":{\"BillingProject\":\"\",\"ClientProtocol\":\"http1\",\"CustomEndpoint\":\"\",\"EnableHttpDnsCache\":false,\"ExperimentalEnableJsonRead\":false,\"GrpcConnPoolSize\":1,\"HttpClientTimeout\":0,\"LimitBytesPerSec\":-1,\"LimitOpsPerSec\":-1,\"MaxConnsPerHost\":0,\"MaxIdleConnsPerHost\":100,\"SequentialReadSizeMb\":200},\"GcsRetries\":{\"ChunkTransferTimeoutSecs\":10,\"MaxRetryAttempts\":0,\"MaxRetrySleep\":30000000000,\"Multiplier\":2,\"ReadStall\":{\"Enable\":true,\"InitialReqTimeout\":20000000000,\"MaxReqTimeout\":1200000000000,\"MinReqTimeout\":1500000000,\"ReqIncreaseRate\":15,\"ReqTargetPercentile\":0.99}},\"ImplicitDirs\":true,\"List\":{\"EnableEmptyManagedFolders\":false},\"Logging\":{\"FilePath\":\"\",\"Format\":\"json\",\"LogRotate\":{\"BackupFileCount\":10,\"Compress\":true,\"MaxFileSizeMb\":512},\"Severity\":\"INFO\"},\"MachineType\":\"\",\"MetadataCache\":{\"DeprecatedStatCacheCapacity\":20460,\"DeprecatedStatCacheTtl\":60000000000,\"DeprecatedTypeCacheTtl\":60000000000,\"EnableNonexistentTypeCache\":false,\"ExperimentalMetadataPrefetchOnMount\":\"disabled\",\"NegativeTtlSecs\":5,\"StatCacheMaxSizeMb\":33,\"TtlSecs\":60,\"TypeCacheMaxSizeMb\":4},\"Metrics\":{\"BufferSize\":256,\"CloudMetricsExportIntervalSecs\":0,\"PrometheusPort\":0,\"StackdriverExportInterval\":0,\"UseNewNames\":false,\"Workers\":3},\"Monitoring\":{\"ExperimentalTracingMode\":\"\",\"ExperimentalTracingSamplingRatio\":0},\"OnlyDir\":\"\",\"Profile\":\"\",\"Read\":{\"BlockSizeMb\":16,\"EnableBufferedRead\":false,\"GlobalMaxBlocks\":40,\"InactiveStreamTimeout\":10000000000,\"MaxBlocksPerHandle\":20,\"MinBlocksPerHandle\":4,\"RandomSeekThreshold\":3,\"StartBlocksPerHandle\":1},\"Write\":{\"BlockSizeMb\":32,\"CreateEmptyFile\":false,\"EnableRapidAppends\":true,\"EnableStreamingWrites\":true,\"FinalizeFileForRapid\":false,\"GlobalMaxBlocks\":4,\"MaxBlocksPerFile\":1}}}\n",
            "{\"timestamp\":{\"seconds\":1760572747,\"nanos\":511372617},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n",
            "/content/gcs_mount\n",
            "chunks.csv\t\tembeddings_768d_3.bin  embeddings_768d_9.bin\n",
            "embeddings_768d_10.bin\tembeddings_768d_4.bin  sample124\n",
            "embeddings_768d_11.bin\tembeddings_768d_5.bin  sample125\n",
            "embeddings_768d_12.bin\tembeddings_768d_6.bin  wiki_chunked.duckdb\n",
            "embeddings_768d_1.bin\tembeddings_768d_7.bin\n",
            "embeddings_768d_2.bin\tembeddings_768d_8.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install numpy==1.26.1\n",
        "!pip install datasets\n",
        "!pip install libomp-dev\n",
        "!pip install faiss-gpu-cu12\n",
        "!pip install chonkie\n",
        "!pip install sentence_transformers\n",
        "!pip install duckdb\n",
        "!pip install openai-agents\n"
      ],
      "metadata": {
        "id": "5JtmZteqD16A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32749c86-d3f6-443c-d852-917a48c9be6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.1 in /usr/local/lib/python3.12/dist-packages (1.26.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer\n",
        "\n"
      ],
      "metadata": {
        "id": "P3HjscDaEMQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#device = torch.device(\"cuda\")\n",
        "model = SentenceTransformer('sentence-transformers/msmarco-roberta-base-v2')\n",
        "wiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ9GutM-Pw-S",
        "outputId": "eb935d39-bfda-4268-bf37-3f6647f2b421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunker = SentenceChunker()\n",
        "\n",
        "#csv_file = \"chunks.csv\"\n",
        "\n",
        "def article_csv():\n",
        "    id = 0\n",
        "    with open(csv_file, \"w\") as f:\n",
        "      summary_list1 = []\n",
        "      for i, row in enumerate(wiki[\"train\"]):\n",
        "\n",
        "          text = row[\"text\"].replace(\"\\n\", \" \")\n",
        "          summary_list1.append(text)\n",
        "          if len(summary_list1) >= 4096:\n",
        "                batched_chunks = chunker.chunk_batch(texts = summary_list1, show_progress=True)\n",
        "                for j in batched_chunks:\n",
        "                    # for each fhunk in article j\n",
        "                    for c in j:\n",
        "                      f.write(f\"{id}\\t{c.text}\\n\")\n",
        "                      id += 1\n",
        "                summary_list1 = []\n",
        "      if summary_list1:\n",
        "\n",
        "          batched_chunks = chunker.chunk_batch(texts = summary_list1, show_progress=True)\n",
        "          for k in batched_chunks:\n",
        "              for c in k:\n",
        "                f.write(f\"{id}\\t{c.text}\\n\")\n",
        "                id += 1\n",
        "article_csv()\n"
      ],
      "metadata": {
        "id": "UgQ4apHf3lMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import duckdb\n",
        "\n",
        "#chunker = SentenceChunker()\n",
        "\n",
        "\n",
        "def article_csv1():\n",
        "\n",
        "  index = faiss.IndexIDMap(faiss.IndexFlatL2(768))\n",
        "\n",
        "\n",
        "  con = duckdb.connect(\"wiki_chunked.duckdb\")\n",
        "\n",
        "  num_rows = int(con.execute(\"SELECT COUNT(column0) FROM wiki_chunked\").fetchone()[0])\n",
        "\n",
        "  last_index = 0\n",
        "  last_save = 0\n",
        "  start_point = 13001280\n",
        "  stop_point = 14000001\n",
        "  steps = 8192\n",
        "  last_row = False\n",
        "\n",
        "  print(f\"There are {num_rows}\")\n",
        "\n",
        "  for i in range(start_point, 13480802, steps):\n",
        "    print(f\"progress {i}\")\n",
        "    last_index = i\n",
        "    if i + 8192 < num_rows:\n",
        "      entries = con.execute(f\"SELECT * FROM wiki_chunked WHERE column0 >= {i} AND column0 < {i+8192}\")\n",
        "    else:\n",
        "      entries = con.execute(f\"SELECT * FROM wiki_chunked WHERE column0 >= {i} AND column0 < {num_rows}\")\n",
        "      last_row = True\n",
        "    entries = entries.fetchall()\n",
        "    print(\"articles retrieved\")\n",
        "    ids = []\n",
        "    text = []\n",
        "    average_size = 0\n",
        "    for e in entries:\n",
        "      #print(type(e[1]))\n",
        "      ids.append(e[0])\n",
        "      average_size += len(e[1])\n",
        "\n",
        "      text.append(e[1])\n",
        "    average_size /= len(entries)\n",
        "    print(\"avg size of entries\", average_size)\n",
        "    print(\"aggregated article lists\")\n",
        "    try:\n",
        "      #assert(len(ids) == len(text))\n",
        "      print(len(text))\n",
        "      print(len(text[0]))\n",
        "\n",
        "      #tokens = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "      embeddings = model.encode(text, show_progress_bar=True, batch_size=16)\n",
        "      #with torch.no_grad():\n",
        "      #  embeddings = model(**tokens).last_hidden_state.mean(dim=1)\n",
        "      #  embeddings.astype(\"float32\")\n",
        "\n",
        "      print(\"embedding complete\")\n",
        "      embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
        "\n",
        "      id_array = np.array(np.array(ids),dtype=np.int64)\n",
        "      index.add_with_ids(embeddings, id_array)\n",
        "      print(\"done adding indexes (not individually)\")\n",
        "    except Exception as e:\n",
        "      for j in range(len(id_array)):\n",
        "        try:\n",
        "          embeddings = model.encode(text[j])\n",
        "          id_array = np.array(np.array(ids[j]),dtype=np.int64)\n",
        "          embeddings = np.ascontiguousarray(embeddings, dtype=np.float32)\n",
        "          index.add_with_ids(embeddings, id_array)\n",
        "        except Exception as e:\n",
        "          print(f\"Bad Chunk at {i}. Skipping...\")\n",
        "    if last_row == True:\n",
        "      break\n",
        "\n",
        "  print(f\"we finished at {last_index + 8192}\")\n",
        "  faiss.write_index(index, \"embeddings_768d_12.bin\")\n",
        "\n",
        "  \"\"\"\n",
        "  if last_index < num_rows:\n",
        "    entries = con.execute(f\"SELECT * FROM wiki_chunked WHERE column0 >= {i} AND column0 < {num_rows}\")\n",
        "\n",
        "  for k in range(last_index, num_rows+1):\n",
        "    try:\n",
        "      embeddings = model.encode(text[i])\n",
        "      id_array = np.array(np.array(id[i]),dtype=np.int64)\n",
        "      index.add_with_ids(embeddings, id_array)\n",
        "    except Exception as e:\n",
        "        print(f\"Bad Chunk at {i}. Skipping...\")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "article_csv1()\n"
      ],
      "metadata": {
        "id": "hBssRCoZra3k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}